{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa7fa05",
   "metadata": {},
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9abc16",
   "metadata": {},
   "source": [
    "- <h3>Predicting the Rating and Gross Collection of movies from IMDB dataset.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dfa5e2",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries and data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "pd.pandas.set_option('display.max_columns',None) # To see all the columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data1\n",
    "data1 = pd.read_csv('CSV1_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eef894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data2\n",
    "data2 = pd.read_csv('CSV2_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f67f72",
   "metadata": {},
   "source": [
    "- Note :- In below dataset ratings mentioned as popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the values of the 'Name' columns\n",
    "if data1['Name'].equals(data2['Name']):\n",
    "    print(\"The values in both column are the same\")\n",
    "else:\n",
    "    print(\"The values in both column are different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data1['Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11032255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces\n",
    "data1['Name'] = data1['Name'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data2['Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces\n",
    "data2['Name'] = data2['Name'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e33542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the values of the 'Name' columns\n",
    "if data1['Name'].equals(data2['Name']):\n",
    "    print(\"The values in both column are the same\")\n",
    "else:\n",
    "    print(\"The values in both column are different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830583fe",
   "metadata": {},
   "source": [
    "- We can see After dealing with wide spaces data1 Name column and data2 Name columns contain same values. So we can use Name column to merge both dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067fab6",
   "metadata": {},
   "source": [
    "# Merging the data1 and data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89def64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(data1, data2, on = \"Name\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae880da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cf23c",
   "metadata": {},
   "source": [
    "- Here we use inner join to merge the datasets so we get the data of the movies whose present in both the datasets data1 and data2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Unnamed: 0_x'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad83715",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Unnamed: 0_y'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c0a8d",
   "metadata": {},
   "source": [
    "- Here we can see both unnamed column has serial number values. We can drop them because it has no use in our dataset. When we stored the data in Excel by default Excel will create serial numbers for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2cac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the information to understand the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07300318",
   "metadata": {},
   "source": [
    "- We have total 1725 rows and 23 columns. 1 column have datatype float64, 3 colums has datatype int64 and 19 columns has object. We need to change datatype of 'Duration of the movie','Metascore','Votes' and 'Gross Collection' because this columns contain integer and float values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80ae99",
   "metadata": {},
   "source": [
    "# Aim :-  Prediction of Gross Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5840d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c57d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the unwanted columns and the columns having maximum None values\n",
    "df.drop(['Unnamed: 0_x'], axis = 1, inplace = True)\n",
    "df.drop(['Unnamed: 0_y'], axis = 1, inplace = True)\n",
    "df.drop(['Director2'], axis = 1 , inplace = True)\n",
    "df.drop(['Director3'], axis = 1 , inplace = True)\n",
    "df.drop(['Director4'], axis = 1 , inplace = True)\n",
    "df.drop(['Director5'], axis = 1 , inplace = True)\n",
    "df.drop(['Director6'], axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296836de",
   "metadata": {},
   "source": [
    "- We have missing values in 'Metascore','Gross Collection','Genre3','Star2','Star3' and 'Star4' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the duplicate values in dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec8dcb",
   "metadata": {},
   "source": [
    "- There is no duplicate values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f94ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65838f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"Duration of the movie\" : \"Duration in min\",\"Popularity\" : \"Ratings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125ce96",
   "metadata": {},
   "source": [
    "## Lets see the unique values in other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Duration in min'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Metascore'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Director1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces\n",
    "df['Director1'] = df['Director1'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349770c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Votes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db441e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Ratings'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ff9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Certificate'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c676513",
   "metadata": {},
   "source": [
    "- Here we can see duration is stored in some places hence we replacing it by 'Not Rated' values\n",
    "- '+' is special character but there is some meaning for 13+ or 16+ certificates in different countries. As we don't have country details we can not replce it, similarly for M/PG also.\n",
    "- https://help.imdb.com/article/contribution/titles/certificates/GU757M8ZJ9ZPXB39?ref_=helpart_nav_27#\n",
    "- By using above link You can see the certificates in different country which awarded for movies in that country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Certificate'] = df['Certificate'].replace({'135 min ':'Not Rated', '128 min ':'Not Rated', '107 min ':'Not Rated', '130 min ':'Not Rated', '155 min ':'Not Rated', '85 min ':'Not Rated', '99 min ':'Not Rated', '97 min ':'Not Rated', '125 min ':'Not Rated' })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces \n",
    "df['Certificate'] = df['Certificate'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d170731",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Genre1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2906993",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Genre2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec15453",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Genre3'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces \n",
    "df['Genre1'] = df['Genre1'].str.strip()\n",
    "df['Genre2'] = df['Genre2'].str.strip()\n",
    "df['Genre3'] = df['Genre3'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47494484",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Star1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special character \"\n",
    "df['Star1'] = df['Star1'].str.replace(r'\"', \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Star2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special character \"\n",
    "df['Star2'] = df['Star2'].str.replace(r'\"', \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Star3'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231af967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special character \"\n",
    "df['Star3'] = df['Star3'].str.replace(r'\"', \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Star4'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special character \"\n",
    "df['Star4'] = df['Star4'].str.replace(r'\"', \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b596d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces \n",
    "df['Star1'] = df['Star1'].str.strip()\n",
    "df['Star2'] = df['Star2'].str.strip()\n",
    "df['Star3'] = df['Star3'].str.strip()\n",
    "df['Star4'] = df['Star4'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some special characters from Gross Collection\n",
    "df['Gross Collection'] = df['Gross Collection'].str.replace(r\"$\", \" \", regex = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gross Collection'] = df['Gross Collection'].str.replace(r\"M\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gross Collection'] = df['Gross Collection'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c46938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Votes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979da3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Votes'] = df['Votes'].str.replace(r\",\", \"\", regex = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Votes'] = df['Votes'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b85131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Metascore'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f119be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Duration in min'] = df['Duration in min'].str.replace(r\"min\", \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e86969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Metascore'] = df['Metascore'].str.replace(r\"Metascore\", \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing spaces\n",
    "df['Metascore'] = df['Metascore'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Metascore'] = df['Metascore'].apply(pd.to_numeric)\n",
    "df['Duration in min'] = df['Duration in min'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeaf7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for data types of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d923daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735b297",
   "metadata": {},
   "source": [
    "- We can see Metascore and Gross Collection columns has null values since its count is less than the total number of rows.\n",
    "\n",
    "\n",
    "- We have a dataset of movies which are relsead from 1924 to 2023.\n",
    "\n",
    "\n",
    "- In Votes we can see the difference in mean and median values means data is not symmetrically distributed in this column. Here mean is greater then median which impies that our data is skewd towords right.\n",
    "\n",
    "\n",
    "- A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. We can see in gross collection and votes column data points are quite spread out from the mean. While in other columns standard deviation indicates that data is close toward mean.\n",
    "\n",
    "\n",
    "- While observing 75% and maximum there may be outliers present in our dataset. But in Metascore column we can see mean and median is nearly equal means in that column data is symmetrically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be63c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check for outliers using IQR\n",
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df['Year'].quantile(0.25)\n",
    "Q3 = df['Year'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df.loc[(df['Year'] < lower_bound) | (df['Year'] > upper_bound)]\n",
    "num_outliers = outliers.shape[0]\n",
    "\n",
    "# print the number of outliers\n",
    "print(f\"Number of outliers: {num_outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dce917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df['Duration in min'].quantile(0.25)\n",
    "Q3 = df['Duration in min'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df.loc[(df['Duration in min'] < lower_bound) | (df['Duration in min'] > upper_bound)]\n",
    "num_outliers = outliers.shape[0]\n",
    "\n",
    "# print the number of outliers\n",
    "print(f\"Number of outliers: {num_outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df['Metascore'].quantile(0.25)\n",
    "Q3 = df['Metascore'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df.loc[(df['Metascore'] < lower_bound) | (df['Metascore'] > upper_bound)]\n",
    "num_outliers = outliers.shape[0]\n",
    "\n",
    "# print the number of outliers\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df['Votes'].quantile(0.25)\n",
    "Q3 = df['Votes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df.loc[(df['Votes'] < lower_bound) | (df['Votes'] > upper_bound)]\n",
    "num_outliers = outliers.shape[0]\n",
    "\n",
    "# print the number of outliers\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69741f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df['Ratings'].quantile(0.25)\n",
    "Q3 = df['Ratings'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df.loc[(df['Ratings'] < lower_bound) | (df['Ratings'] > upper_bound)]\n",
    "num_outliers = outliers.shape[0]\n",
    "\n",
    "# print the number of outliers\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc80e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df['Gross Collection'].quantile(0.25)\n",
    "Q3 = df['Gross Collection'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df.loc[(df['Gross Collection'] < lower_bound) | (df['Gross Collection'] > upper_bound)]\n",
    "num_outliers = outliers.shape[0]\n",
    "\n",
    "# print the number of outliers\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57139aa6",
   "metadata": {},
   "source": [
    "- Metascore column has no outliers while Votes and Gross Collection columns has maximum outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71e9d7",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the top 5 voted movies using bivariate analysis\n",
    "top_voted = df.sort_values(['Votes'], ascending = False)\n",
    "plt.figure(figsize=(25, 10))\n",
    "g=sns.barplot(x=top_voted['Name'][:5],y=top_voted['Votes'][:5], palette = 'hls')\n",
    "g.set_title(\"Top Voted Movies\", weight = \"bold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c954a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the top 5 voted movies ratings\n",
    "plt.figure(figsize=(25, 10))\n",
    "g=sns.barplot(x=top_voted['Name'][:5],y=top_voted['Ratings'][:5], palette = 'husl')\n",
    "g.set_title(\"IMDB Rating of top voted movies\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the metascore of this top 5 voted movies\n",
    "plt.figure(figsize=(25, 10))\n",
    "g=sns.barplot(x=top_voted['Name'][:5],y=top_voted['Metascore'][:5], palette = 'husl')\n",
    "g.set_title(\"Metascore of top rated movies\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the Gross Collection of this top 5 voted movies\n",
    "plt.figure(figsize=(25, 10))\n",
    "g=sns.barplot(x=top_voted['Name'][:5],y=top_voted['Gross Collection'][:5], palette = 'husl')\n",
    "g.set_title(\"Gross collection by top rated movies\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc7e62",
   "metadata": {},
   "source": [
    "- The top voted five movies are 'The Dark Knight', 'Inception', ' The Matrix Name', 'The Lord of the Rings - The Fellowship of the Ring', And 'The Lord of the Rings - The Return of the king'.\n",
    "- All five top voted movies ratated above 8.\n",
    "- Also we can see this top voted movies metascore grater than 60. \n",
    "- If we see this top voted movies by earnings then the 'The Dark Knight' is in first position. And all other movies has earning greater than 150 Million dollers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aadf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the top movies by highest earnings\n",
    "highest_earning = df.sort_values(['Gross Collection'], ascending = False)\n",
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.barplot(x=highest_earning['Name'][:5],y=highest_earning['Gross Collection'][:5], palette = 'husl')\n",
    "g.set_title(\"Movies with highest Gross (earning)\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the top movies by ratings\n",
    "plt.figure(figsize=(20, 9))\n",
    "highest_rating = df.sort_values(['Ratings'], ascending = False)\n",
    "g=sns.barplot(x=highest_rating['Name'][:5],y=highest_rating['Ratings'][:5], palette = 'husl')\n",
    "g.set_title(\"Movies with highest highest_rating\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afb1f6",
   "metadata": {},
   "source": [
    "- 'Star Wars Episode VII The Force Awakens', 'Avengers Endgame', 'Spider Man No Way Home Name', 'Avatar', 'Top Gun Maverick' movies has highest earning but they are not in the list of top voted movies.\n",
    "- Also we can observe top voted movies gross collection is not more than 600 million dollers but they still in the list of top votes and top rated 5 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5dedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the Maximum movies released years using Univariate analysis\n",
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.barplot(x=df['Year'].value_counts()[:10].index,y=df['Year'].value_counts()[:10])\n",
    "g.set_title(\"Maximum movies released in-\", weight = \"bold\")\n",
    "g.set_xlabel(\"Years\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ac901",
   "metadata": {},
   "source": [
    "- In the year of 2016, 2014,2013 and 2011 maximum number of movies are realesed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.barplot(x=df['Director1'].value_counts().index[0:10],y=df['Director1'].value_counts()[:10])\n",
    "g.set_title(\"Mostly Occurred Directors\", weight = \"bold\")\n",
    "g.set_xlabel(\"Directors\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6579fd",
   "metadata": {},
   "source": [
    "- Director 'Michael Bay' directed maximum number of movies i.e 13+ movies than other directors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a162a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the Stars with most Occurences in movies using multivariate analysis\n",
    "stars=['Star1','Star2','Star3','Star4']\n",
    "fig,axs=plt.subplots(4,1,figsize=(20,7))\n",
    "ax=0\n",
    "for x in stars:\n",
    "    axs[ax].bar(df[x].value_counts()[:10].index,df[x].value_counts()[:10],color = 'Gray')\n",
    "    axs[ax].set_title(x)\n",
    "    axs[ax].set_ylabel(\"Appearances\", weight = \"bold\")\n",
    "    ax+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the appearances of Stars in Top voted movies\n",
    "stars=['Star1','Star2','Star3','Star4']\n",
    "fig,axs=plt.subplots(4,1,figsize=(20,7))\n",
    "ax=0\n",
    "for x in stars:\n",
    "    s=df.groupby([x]).sum().reset_index()\n",
    "    d=s.sort_values(['Votes'],ascending=False)[:10]\n",
    "    axs[ax].bar(d[x],d['Votes'],color = 'Gray')\n",
    "    axs[ax].set_title(x)\n",
    "    axs[ax].set_ylabel(\"Appearances\", weight = \"bold\")\n",
    "    ax+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the appearances of Stars in top grossed movies\n",
    "stars=['Star1','Star2','Star3','Star4']\n",
    "fig,axs=plt.subplots(4,1,figsize=(20,7))\n",
    "ax=0\n",
    "for x in stars:\n",
    "    s=df.groupby([x]).sum().reset_index()\n",
    "    d=s.sort_values(['Gross Collection'],ascending=False)[:10]\n",
    "    axs[ax].bar(d[x],d['Gross Collection'],color = 'Gray')\n",
    "    axs[ax].set_title(x)\n",
    "    axs[ax].set_ylabel(\"Appearances\", weight = \"bold\")\n",
    "    ax+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the appearances of Stars in top rated movies\n",
    "stars=['Star1','Star2','Star3','Star4']\n",
    "fig,axs=plt.subplots(4,1,figsize=(20,7))\n",
    "ax=0\n",
    "for x in stars:\n",
    "    s=df.groupby([x]).sum().reset_index()\n",
    "    d=s.sort_values(['Ratings'],ascending=False)[:10]\n",
    "    axs[ax].bar(d[x],d['Ratings'], color = 'Gray')\n",
    "    axs[ax].set_title(x)\n",
    "    axs[ax].set_ylabel(\"Appearances\", weight = \"bold\")\n",
    "    ax+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d644b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the appearances of Stars in top meta_scores\n",
    "stars=['Star1','Star2','Star3','Star4']\n",
    "fig,axs=plt.subplots(4,1,figsize=(17,7))\n",
    "ax=0\n",
    "for x in stars:\n",
    "    s=df.groupby([x]).sum().reset_index()\n",
    "    d=s.sort_values(['Metascore'],ascending=False)[:10]\n",
    "    axs[ax].bar(d[x],d['Metascore'],color = 'Gray')\n",
    "    axs[ax].set_title(x)\n",
    "    axs[ax].set_ylabel(\"Appearances\", weight = \"bold\")\n",
    "    ax+=1\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3d1f0",
   "metadata": {},
   "source": [
    "- The star 'Samuel L. Jackson' working on lots of movies and people rated his movies with good ratings and metascore.\n",
    "- According to Gross collection and votings star 'Robert Downey Jr.' is in highest position from all other stars. \n",
    "- People gives maximum votes for his movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.countplot(df['Certificate'])\n",
    "g.set_title(\"Count of Certificates provided\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0ec61",
   "metadata": {},
   "source": [
    "- Mostaly achieved certificates by movies are 'PG 13', 'R' and 'PG'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "genre_counts = pd.concat([df['Genre1'], df['Genre2'], df['Genre3']]).value_counts()\n",
    "plt.bar(genre_counts.index, genre_counts.values)\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Movie Genres')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_genre1 = df.groupby('Genre1')\n",
    "df_by_genre2 = df.groupby('Genre2')\n",
    "df_by_genre3 = df.groupby('Genre3')\n",
    "agg_genre1=df_by_genre1.sum()\n",
    "agg_genre2=df_by_genre2.sum()\n",
    "agg_genre3=df_by_genre3.sum()\n",
    "ge1_ge2 = agg_genre1.add(agg_genre2,fill_value=0.0)# fill the missing values with 0.0 before adding the two columns.\n",
    "final_genre = ge1_ge2.add(agg_genre3,fill_value=0.0)# fill the missing values with 0.0 before adding the two columns.\n",
    "genre_temp=final_genre.reset_index().head(10)\n",
    "genre_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f8ef5",
   "metadata": {},
   "source": [
    "- Action is Genre which found is Maximum movies.\n",
    "- After grouping and adding all the Genre we observe that Action has the maximum Revenue, Ratings, Votes and Metascore.\n",
    "- Adventure movies are on second position according to Votes, Rating and Gross Collection.\n",
    "- Peoples also like to watch Comedy,Crime,Drama and Fantasy movies but Crime movies Revenue is less than other three Genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the duration of movies \n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.histplot(data=df, x='Duration in min', bins=20, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f964b",
   "metadata": {},
   "source": [
    "- Most of Movies Belongs to Duration Range between 80 to 130 Minutes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a54d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Metascore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Metascore'] = df['Metascore'].interpolate().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f21fed",
   "metadata": {},
   "source": [
    "- We use interpolation method because it is a method of estimating values between two known values based on some assumed relationship between the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Metascore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Genre2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Genre3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe587a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Genre2'].fillna(df['Genre2'].mode()[0],inplace = True)\n",
    "df['Genre3'].fillna(df['Genre3'].mode()[0],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5030b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Star2'].fillna(df['Star2'].mode()[0],inplace = True)\n",
    "df['Star3'].fillna(df['Star3'].mode()[0],inplace = True)\n",
    "df['Star4'].fillna(df['Star4'].mode()[0],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e5a8a",
   "metadata": {},
   "source": [
    "## As Gross Collection is our target coulmn and it has some null values before taking a decision to remove it lets analyse that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77af6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data in missing values whose Gross Collection is null\n",
    "missing_values = df.loc[df['Gross Collection'].isna()]\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f36c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d093b",
   "metadata": {},
   "source": [
    "- The year of the movie has a weak negative correlation with the metascore and ratings of the movie, which suggests that older movies may be less highly rated than newer movies.\n",
    "\n",
    "- The duration of the movie has a weak positive correlation with the metascore, ratings, which suggests that longer movies may be more highly rated and more successful at the box office.\n",
    "\n",
    "- The metascore and ratings have a strong positive correlation, which suggests that they may be measuring similar aspects of the movie's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96666a",
   "metadata": {},
   "source": [
    "- If we drop values accrodingly Gross Collection then we are lossing the lots of data of the movies which are released in between 2016 to 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69006c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.countplot(missing_values['Certificate'])\n",
    "g.set_title(\"Count of Certificates provided\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bbe2d",
   "metadata": {},
   "source": [
    "- According to our previous graph of Certificate we have maximum count for PG 13 and R certificates and all other certificates has less count. If we drop missing_values data then We loss details of movies which awarded by certificates like PG 13, PG, R which has maximum count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d31dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values['Genre1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddeaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values['Genre2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f927be",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values['Genre3'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4a51a",
   "metadata": {},
   "source": [
    "- If we drop the missing_values data we loss the high count of Action, Drama and Adventure Genres.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052ac0e",
   "metadata": {},
   "source": [
    "- If we loss 15.30% of data then we loss lots of important information. Droping the missing values accordingly target coulumn is not a right decision in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Gross Collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values using linear interpolation\n",
    "df['Gross Collection'] = df['Gross Collection'].interpolate(method='linear')\n",
    "\n",
    "# Round values to 2 decimal places\n",
    "df['Gross Collection'] = round(df['Gross Collection'], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8172a",
   "metadata": {},
   "source": [
    "- We use interpolation method because it is a method of estimating values between two known values based on some assumed relationship between the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798378a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Gross Collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of data which is useful for prediction of Ratings\n",
    "New_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "# Create scatter plot for Genre1\n",
    "plt.scatter(df['Genre1'], df['Gross Collection'], alpha=0.5, label='Genre1')\n",
    "\n",
    "# Create scatter plot for Genre2\n",
    "plt.scatter(df['Genre2'], df['Gross Collection'], alpha=0.5, label='Genre2')\n",
    "\n",
    "# Create scatter plot for Genre3\n",
    "plt.scatter(df['Genre3'], df['Gross Collection'], alpha=0.5, label='Genre3')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Gross Collection')\n",
    "plt.title('Scatter Plot of Genres vs. Gross Collection')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345a8a5",
   "metadata": {},
   "source": [
    "- By observing above graphs we can see that Gross Collection of Genre not varies according to the Genre. But here we are not taking the decision to drop genre even its not showing good relation with Gross Collection. Because Certain genres tend to be more popular with certain demographics or during specific times of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the correlation \n",
    "plt.figure(figsize=(20,9))\n",
    "sns.heatmap(df.corr(),annot= True,linewidths=1,fmt=' .2f',cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56799b94",
   "metadata": {},
   "source": [
    "- There is a weak positive correlation (0.13) between Year and Duration in min, suggesting that movies released in more recent years tend to be slightly longer.\n",
    "- There is a weak negative correlation (-0.14) between Year and Ratings, suggesting that movies released in more recent years tend to be slightly lower rated on average.\n",
    "- There is a positive correlation between Duration in min and all other variables, with the strongest correlation being with Ratings (0.34), suggesting that longer movies tend to receive higher ratings, more votes, and make more money.\n",
    "- There is a positive correlation between Votes and Gross Collection (0.68), suggesting that more popular movies tend to make more money.\n",
    "- There is a positive correlation between Ratings and Gross Collection (0.28), suggesting that movies that are well-rated on IMDb tend to make more money.\n",
    "- There is a weak positive correlation (0.26) between Metascore and Gross Collection, suggesting that movies with higher scores on Metascore tend to make slightly more money.\n",
    "- There is a positive correlation between Duration in min and Gross Collection (0.33), suggesting that longer movies tend to make more money. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.barplot(x=df['Certificate'],y=df['Gross Collection'])\n",
    "g.set_title(\"Certificate and Gross Collection\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae68f1d",
   "metadata": {},
   "source": [
    "- PG 13,PG,13+ certificates awarded movies has a highest Gross Collection than other certificates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63383914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sorted list of directors by frequency count\n",
    "sorted_directors = df['Director1'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Group the DataFrame by Director1 and calculate the sum of Gross Collection for each director\n",
    "director_gross = df.groupby('Director1')['Gross Collection'].sum()\n",
    "\n",
    "# Select only the directors whose names are in the sorted_directors index\n",
    "sorted_director_gross = director_gross.loc[sorted_directors.index]\n",
    "\n",
    "# Display the sorted list of directors with their gross collection\n",
    "sorted_director_gross\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a50031",
   "metadata": {},
   "source": [
    "- We can see Director who directed maximum movies does not mean achive the maximum Gross Collection for thier movies. Gross Collection is not showing strong relation with Director."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54fbb2",
   "metadata": {},
   "source": [
    "# Encoding the Categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "# Define the target column\n",
    "target_col = \"Gross Collection\"\n",
    "\n",
    "# Define the categorical columns to be encoded\n",
    "cat_cols = [\"Genre1\", \"Genre2\", \"Genre3\", \"Star1\", \"Star2\", \"Star3\", \"Star4\"]\n",
    "\n",
    "# Create an instance of TargetEncoder and fit it on the data\n",
    "te = ce.TargetEncoder(cols=cat_cols)\n",
    "te.fit(df, df[target_col])\n",
    "\n",
    "# Apply the encoding on the categorical columns\n",
    "df_encoded = te.transform(df)\n",
    "\n",
    "# Print the encoded DataFrame\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456a9f1",
   "metadata": {},
   "source": [
    "- TargetEncoder is a type of categorical encoding technique that encodes each category in a categorical variable based on the mean target value of the observations in that category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de215e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ddca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To encode Name column lets check the unique values in name column\n",
    "print(len(list(df_encoded['Name'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85173318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(df_encoded['Director1'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "# Encode the Director1 columns using FrequencyEncoder\n",
    "fe = ce.CountEncoder(cols=[\"Director1\"])\n",
    "df_encoded = fe.fit_transform(df_encoded)\n",
    "\n",
    "# Print the encoded DataFrame\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57b075",
   "metadata": {},
   "source": [
    "- Frequency encoding is a technique that encodes each category in a categorical variable based on the frequency (or count) of observations in that category. This technique can be used even if the variable has a large number of unique values, and will create a new feature for each category in the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccc797",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(df_encoded.Certificate.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create an instance of LabelEncoder and fit it on the data\n",
    "le = LabelEncoder()\n",
    "df_encoded['Certificate'] = le.fit_transform(df_encoded['Certificate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create an instance of LabelEncoder and fit it on the data\n",
    "le = LabelEncoder()\n",
    "df_encoded['Name'] = le.fit_transform(df_encoded['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e443ae",
   "metadata": {},
   "source": [
    "- LabelEncoder is a simple and effective method for encoding categorical variables that have a small number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f69194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv('df_encoded.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbd911",
   "metadata": {},
   "source": [
    "- Now we have successfully encode all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the correlation \n",
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(df_encoded.corr(),annot= True,linewidths=1,fmt=' .2f',cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88334b1f",
   "metadata": {},
   "source": [
    "- Here Name, Director1 and Genre1 columns shows very weak correlation with dependent and independent features.\n",
    "- Even Genre1 column is also showing very weak correlation but Genres are divided into three parts droping one column is not a good dicision because in above analysis we saw that Action Genre has Maximim Gross collection than other Genres and Action Genre count is maximum in Genre1 column dropping it may loss the important information. \n",
    "- Here we drop Name because they are not adding that much values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b025b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(['Director1','Name'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Create a new dataframe with all the features (numeric and categorical)\n",
    "X = df_encoded[['Year','Duration in min','Votes','Ratings','Metascore']]\n",
    "\n",
    "# Calculate VIF values for each featur\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print the VIF values for each feature\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Create a new dataframe with all the features (numeric and categorical)\n",
    "X = df_encoded[['Duration in min','Votes','Year','Metascore']]\n",
    "\n",
    "# Calculate VIF values for each feature\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print the VIF values for each feature\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Create a new dataframe with all the features (numeric and categorical)\n",
    "X = df_encoded[['Votes','Duration in min','Metascore']]\n",
    "\n",
    "# Calculate VIF values for each feature\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print the VIF values for each feature\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d30aa7",
   "metadata": {},
   "source": [
    "- Here we choose to drop Rating and Metascore to avoid multicollinearity. Also Year showing very weak correlation with dependent and independent features hence we choose to drop it. Again We have very high VIF but duration shows quit high positive correlation with target column so we are not choose to drop any other feature right now. Also we are not getting strong relation with certificate and gross collection hence choose to drop that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74514323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(['Metascore','Year','Certificate','Ratings'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6f128",
   "metadata": {},
   "source": [
    "# Ckeking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df_encoded['Votes'].quantile(0.25)\n",
    "Q3 = df_encoded['Votes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df_encoded.loc[(df['Votes'] < lower_bound) | (df_encoded['Votes'] > upper_bound)]\n",
    "outliers['Votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5593e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df_encoded['Duration in min'].quantile(0.25)\n",
    "Q3 = df_encoded['Duration in min'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df_encoded.loc[(df['Duration in min'] < lower_bound) | (df_encoded['Duration in min'] > upper_bound)]\n",
    "outliers['Duration in min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c07eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the interquartile range (IQR)\n",
    "Q1 = df_encoded['Gross Collection'].quantile(0.25)\n",
    "Q3 = df_encoded['Gross Collection'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the upper and lower bounds for outliers\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "\n",
    "# count the number of outliers\n",
    "outliers = df_encoded.loc[(df['Gross Collection'] < lower_bound) | (df_encoded['Gross Collection'] > upper_bound)]\n",
    "outliers['Gross Collection']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38d39a",
   "metadata": {},
   "source": [
    "- Here We are not dealing with outliers.\n",
    "- We can see if we remove the ouliers we loss the maximum Gross Collections, Durations and Votes which will be a good factor for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdc009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the skewness \n",
    "df_encoded.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92cea54",
   "metadata": {},
   "source": [
    "- Here we can see Ratings and Certificates are skewed towords left and other columns skewd toword right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into features and vectors.\n",
    "X=df_encoded.drop(['Gross Collection'], axis=1)\n",
    "y=df_encoded['Gross Collection'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808212b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.log(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "skewness = skew(X)\n",
    "print(\"Skewness:\", skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a distplot\n",
    "sns.distplot(X, kde=True, hist=True)\n",
    "\n",
    "# set the title and axis labels\n",
    "plt.title('Distribution Plot')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test= train_test_split(X,y,train_size=0.80,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0421fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333653c6",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into X and y variables here\n",
    "\n",
    "# Define a range of random state values to try\n",
    "random_states = np.arange(100)\n",
    "\n",
    "# Initialize variables to keep track of the best model and its performance\n",
    "best_r2_score = -np.inf\n",
    "best_random_state = None\n",
    "best_model = None\n",
    "\n",
    "# Loop through all the random state values to find the best one\n",
    "for random_state in random_states:\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # Fit a linear regression model to the training data\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute the R-squared score for this model\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Check if this model has a better R-squared score than the previous best model\n",
    "    if r2 > best_r2_score:\n",
    "        best_r2_score = r2\n",
    "        best_random_state = random_state\n",
    "        best_model = model\n",
    "\n",
    "# Print the best random state and its corresponding R-squared score\n",
    "print(\"Best random state:\", best_random_state)\n",
    "print(\"Best R-squared score:\", best_r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c117b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "best_cv = None\n",
    "best_score = 0\n",
    "\n",
    "for cv in range(3, 11):\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
    "    cv_score = cv_scores.mean()\n",
    "    print('{} at this fold score is {}'.format(cv,cv_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df362c",
   "metadata": {},
   "source": [
    "- We can see at 10 fold we get best r2 score than other fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ac219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ecd6a",
   "metadata": {},
   "source": [
    "# Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a053bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "\n",
    "# Fit a linear regression model\n",
    "lasso_reg = lasso.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "lasso_y_pred = lasso.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test,lasso_y_pred)\n",
    "\n",
    "print(\"R2 Score\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(lasso, X, y, cv=10, scoring='r2')\n",
    "# Print the mean and standard deviation of the mean squared error\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1983b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, lasso_y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922e8b1",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Define Ridge model with cross-validation\n",
    "ridge = Ridge()\n",
    "\n",
    "# Fit a linear regression model\n",
    "ridge_reg = ridge.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "ridge_y_pred = ridge.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, ridge_y_pred)\n",
    "\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77887d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(ridge,X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "\n",
    "print(cv_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, ridge_y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75b737",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Define KNeighborsRegressor model with cross-validation\n",
    "knn = KNeighborsRegressor()\n",
    "knn_reg = knn.fit(x_train,y_train)\n",
    "# Predict on test data\n",
    "knn_y_pred = knn.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, knn_y_pred)\n",
    "\n",
    "print('R2 score', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53417fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(knn, X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, knn_y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1691ae",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define the SVR model\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "# Fit the model on the training data\n",
    "svr.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "svr_y_pred = svr.predict(x_test)\n",
    "\n",
    "# Compute the R2 score for this model\n",
    "r2 = r2_score(y_test, svr_y_pred)\n",
    "\n",
    "# Print the R2 score\n",
    "print(\"R2 score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(svr, X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac102068",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, svr_y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe9e9a",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt=DecisionTreeRegressor()\n",
    "dt_reg = dt.fit(x_train, y_train)\n",
    "dt_y_pred=dt.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, dt_y_pred)\n",
    "\n",
    "print(\"R2 score\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(dt, X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd33565",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, dt_y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2a50a",
   "metadata": {},
   "source": [
    "# Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define Random Forest Regressor model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Fit the model on training data\n",
    "rf_reg = rf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "rf_y_pred = rf.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "print(\"R2 score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(knn, X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71176e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, rf_y_pred)\n",
    "plt.title('Actual vs. Predicted Values (R2 = {:.2f})'.format(r2))\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d413d",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af117c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define Gradient Boosting model with cross-validation\n",
    "gb = GradientBoostingRegressor(random_state=2)\n",
    "gb_reg = gb.fit(x_train, y_train)\n",
    "gb_y_pred = gb.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, gb_y_pred)\n",
    "\n",
    "print(\"R2 score:\", r2)\n",
    "\n",
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(gb, X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "print(\"cv_mean\",cv_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df0520",
   "metadata": {},
   "source": [
    "- For R2 score, a higher value indicates a better model fit. R2 score measures how well the model explains the variance in the target variable. So, if we have an R2 score, the cross-validation score to be close to the R2 score or at least not significantly lower than the R2 score.\n",
    "- Gradient Boosting shows less difference in r2 score and cv score. Lets do the hyper parameter tuning on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1209b4",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning for Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a996296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'learning_rate': uniform(0.01, 0.5),\n",
    "    'max_depth': randint(1, 10),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform random search cross-validation to find the best hyperparameters\n",
    "gb_random = RandomizedSearchCV(gb, param_distributions=param_grid, cv=10, scoring='r2', n_iter=50, random_state=2)\n",
    "gb_random.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and their corresponding R2 score\n",
    "print(\"Best hyperparameters:\", gb_random.best_params_)\n",
    "print(\"Best R2 score:\", gb_random.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define HistGradientBoostingRegressor model\n",
    "hgb = HistGradientBoostingRegressor(random_state=2)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [1, 3, 5, 9],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation to find the best hyperparameters\n",
    "hgb_grid = GridSearchCV(hgb, param_grid, cv=10, scoring='r2')\n",
    "hgb_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and their corresponding R2 score\n",
    "print(\"Best hyperparameters:\", hgb_grid.best_params_)\n",
    "print(\"Best R2 score:\", hgb_grid.best_score_)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "hgb_y_pred = hgb_grid.predict(x_test)\n",
    "r2 = r2_score(y_test, hgb_y_pred)\n",
    "print(\"R2 score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using cross-validation\n",
    "cv_scores = cross_val_score(hgb, X, y, cv=10, scoring='r2')\n",
    "cv_mean = np.mean(cv_scores)\n",
    "print(\"cv_mean\",cv_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46ac8e",
   "metadata": {},
   "source": [
    "- The corresponding R2 score of the model using these hyperparameters is 0.636, which is the best R2 score achieved by any combination of hyperparameters during the hyperparameter tuning process.\n",
    "\n",
    "\n",
    "- Then the model was used to make predictions on the test set, and the R2 score was found to be 0.735. This suggests that the model has a good performance on the test set, with an R2 score of 0.735 indicating that 73.5% of the variance in the target variable can be explained by the model.\n",
    "\n",
    "\n",
    "- In cases where the dataset contains outliers and it is not desirable to remove them, R2 score can be a more suitable metric than mean squared error (MSE). R2 score measures the proportion of variance in the target variable that is explained by the model, and is less sensitive to outliers than MSE.\n",
    "\n",
    "\n",
    "- HistGradientBoostingRegressor is a gradient boosting algorithm for regression tasks, introduced in Scikit-learn version 0.23. It is based on histograms and gradient boosting, which makes it very fast and scalable for large datasets. Instead of using decision trees, HistGradientBoostingRegressor builds histograms of the features, and computes the gradients based on these histograms. It also uses the L2 regularization to avoid overfitting.\n",
    "\n",
    "\n",
    "- In our case after doing hyper parameter r2 score get reduced hence we used HistGradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e618e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(hgb_grid.best_estimator_, 'best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d0a6b",
   "metadata": {},
   "source": [
    "#  To Predict Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5273b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the correlation \n",
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(New_df.corr(),annot= True,linewidths=1,fmt=' .2f',cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce90dc",
   "metadata": {},
   "source": [
    "- Year column showing weak correlation with dependent and independent column hence choose to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447206fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_df.drop('Year',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5994952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sorted list of directors by frequency count\n",
    "sorted_directors = New_df['Director1'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Group the DataFrame by Director1 and calculate the sum of Gross Collection for each director\n",
    "director_rating = New_df.groupby('Director1')['Ratings'].sum()\n",
    "\n",
    "# Select only the directors whose names are in the sorted_directors index\n",
    "sorted_director_rating = director_rating.loc[sorted_directors.index]\n",
    "\n",
    "# Display the sorted list of directors with their gross collection\n",
    "sorted_director_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8755f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "g=sns.barplot(x=New_df['Certificate'],y=New_df['Ratings'])\n",
    "g.set_title(\"Certificate and Gross Collection\", weight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f76070",
   "metadata": {},
   "source": [
    "- We can see movies rating are showing strong relation with certificates. If movie is not awarded by any certificate then also its rating is greater than 6.5 which we can say medium rating according to range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b345a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "# Create scatter plot for Genre1\n",
    "plt.scatter(df['Genre1'], df['Ratings'], alpha=0.5, label='Genre1')\n",
    "\n",
    "# Create scatter plot for Genre2\n",
    "plt.scatter(df['Genre2'], df['Ratings'], alpha=0.5, label='Genre2')\n",
    "\n",
    "# Create scatter plot for Genre3\n",
    "plt.scatter(df['Genre3'], df['Ratings'], alpha=0.5, label='Genre3')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Ratings')\n",
    "plt.title('Scatter Plot of Genres vs. Ratings')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8134c",
   "metadata": {},
   "source": [
    "- We can see here Genre shows good relationship with ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "# Define the target column\n",
    "target_col = \"Ratings\"\n",
    "\n",
    "# Define the categorical columns to be encoded\n",
    "cat_cols = [\"Genre1\", \"Genre2\", \"Genre3\", \"Star1\", \"Star2\", \"Star3\", \"Star4\"]\n",
    "\n",
    "# Create an instance of TargetEncoder and fit it on the data\n",
    "te = ce.TargetEncoder(cols=cat_cols)\n",
    "te.fit(New_df, New_df[target_col])\n",
    "\n",
    "# Apply the encoding on the categorical columns\n",
    "df_encoded = te.transform(New_df)\n",
    "\n",
    "# Print the encoded DataFrame\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52551668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "# Encode the Name and Director1 columns using FrequencyEncoder\n",
    "fe = ce.CountEncoder(cols=[\"Director1\"])\n",
    "df_encoded = fe.fit_transform(df_encoded)\n",
    "\n",
    "# Print the encoded DataFrame\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create an instance of LabelEncoder and fit it on the data\n",
    "le = LabelEncoder()\n",
    "df_encoded['Certificate'] = le.fit_transform(df_encoded['Certificate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['Name'] = le.fit_transform(df_encoded['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2009c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the correlation \n",
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(df_encoded.corr(),annot= True,linewidths=1,fmt=' .2f',cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(['Name','Certificate'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500715e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Create a new dataframe with all the features (numeric and categorical)\n",
    "X = df_encoded[['Duration in min','Votes','Metascore','Gross Collection']]\n",
    "\n",
    "# Calculate VIF values for each featur\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print the VIF values for each feature\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bac330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Create a new dataframe with all the features (numeric and categorical)\n",
    "X = df_encoded[['Votes','Metascore','Gross Collection']]\n",
    "\n",
    "# Calculate VIF values for each featur\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print the VIF values for each feature\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop('Duration in min',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_encoded['Ratings'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb5ef80",
   "metadata": {},
   "source": [
    "- Here we convert choose to convert certain ranges into classes and try to solve this problem using classification to reduce the complexity of problem which may occur during prediction if we solve it by regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rating ranges and corresponding labels\n",
    "ranges = [(1.0, 1.9), (2.0, 2.9), (3.0, 3.9), (4.0, 4.9), (5.0, 5.9), (6.0, 6.9), (7.0, 7.9), (8.0, 8.9), (9.0, 9.9)]\n",
    "labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Create a function to assign labels based on the rating ranges\n",
    "def assign_label(rating):\n",
    "    for i, r in enumerate(ranges):\n",
    "        if rating >= r[0] and rating <= r[1]:\n",
    "            return labels[i]\n",
    "    return None\n",
    "\n",
    "# Apply the function to create a new column with the labels\n",
    "df_encoded['Ratings'] = df_encoded['Ratings'].apply(assign_label)\n",
    "\n",
    "# Print the resulting DataFrame with labels\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(f):\n",
    "    df_encoded[f].plot.box()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot('Metascore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e166af",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot('Votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot('Gross Collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca7102",
   "metadata": {},
   "source": [
    "# Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z= np.abs(zscore(df_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold= 3 \n",
    "print(np.where(z>3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_encoded[(z<3).all(axis=1)]\n",
    "print(df_encoded.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loss = (1725-1453)/1725*100\n",
    "data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into features and vectors.\n",
    "X=df2.drop(['Ratings'], axis=1)\n",
    "y=df2['Ratings'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22dfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.sqrt(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler object and fit to data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform data\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "skewness = skew(X)\n",
    "print(\"Skewness:\", skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8780de",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skew(X)\n",
    "print(\"Skewness:\", skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a distplot\n",
    "sns.distplot(X, kde=True, hist=True)\n",
    "\n",
    "# set the title and axis labels\n",
    "plt.title('Distribution Plot')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_score = 0\n",
    "best_rs = 0\n",
    "\n",
    "# Loop over different random states to find the best one\n",
    "for rs in range(100):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "\n",
    "    # Create a logistic regression model and fit it to the training data\n",
    "    clf = LogisticRegression(random_state=rs)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Update the best score and random state if necessary\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_rs = rs\n",
    "\n",
    "print(f'Best random state: {best_rs}, Best test score: {best_score:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='macro',zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average='macro',zero_division=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 score: {f1:.3f}')\n",
    "print('Confusion matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352cf8f",
   "metadata": {},
   "source": [
    "- The overall accuracy of the model is 0.557, which is not very high, indicating that the model is not very accurate in predicting the correct class labels.\n",
    "- The precision score is 0.394, which means that when the model predicts a class label, it is correct 39.4% of the time on average across all classes.\n",
    "- The recall score is 0.562, which means that the model correctly identifies only 56.2% of the positive class instances across all classes.\n",
    "- The F1 score is 0.394, which is a harmonic mean of precision and recall scores, indicating that the model is not performing well on both metrics.\n",
    "- Overall, while the model may have some predictive power, it is not performing very well on this dataset. Further analysis and improvement of the model may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Create a logistic regression model\n",
    "clf = LogisticRegression(random_state=26)\n",
    "\n",
    "# Perform cross-validation and compute the mean accuracy score\n",
    "for cv in range(3,9):\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=cv,scoring = 'accuracy')\n",
    "    mean_cv_score = cv_scores.mean()\n",
    "    print(\"{} at this fold cv_score is {}\".format(cv,mean_cv_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520395d",
   "metadata": {},
   "source": [
    "- At fold 8 we get good accuracy hence we choose 8 as a best fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=26)\n",
    "\n",
    "# Balance the classes on the training set only\n",
    "ros = RandomOverSampler(random_state=26)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ed32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier and fit it to the training data\n",
    "clf = DecisionTreeClassifier(random_state=26)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and compute the mean accuracy score\n",
    "cv_scores = cross_val_score(clf, X, y, cv=8, scoring='accuracy')\n",
    "mean_cv_score = cv_scores.mean()\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73b818",
   "metadata": {},
   "source": [
    "# Random Forect Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a random forest classifier and fit it to the training data\n",
    "rfc = RandomForestClassifier(random_state=26)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = rfc.score(X_test, y_test)\n",
    "y_pred = rfc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d883069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and compute the mean accuracy score\n",
    "cv_scores = cross_val_score(rfc, X, y, cv=8, scoring='accuracy')\n",
    "mean_cv_score = cv_scores.mean()\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e44ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create a Support vector classifier and fit it to the training data\n",
    "svc = SVC(random_state=26)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = svc.score(X_test, y_test)\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and compute the mean accuracy score\n",
    "cv_scores = cross_val_score(svc, X, y, cv=8, scoring='accuracy')\n",
    "mean_cv_score = cv_scores.mean()\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2179729",
   "metadata": {},
   "source": [
    "# KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = knn.score(X_test, y_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and compute the mean accuracy score\n",
    "cv_scores = cross_val_score(knn, X, y, cv=8, scoring='accuracy')\n",
    "mean_cv_score = cv_scores.mean()\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74076f2",
   "metadata": {},
   "source": [
    "- As comparing other models Random Forest model perform well. We coose random forest as a best model based on matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rfc = RandomForestClassifier(random_state=26)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform a grid search with cross-validation\n",
    "grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=8, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Print the accuracy score of the best model\n",
    "print('Accuracy score:', grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107de10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# train your best model and save it\n",
    "best_model = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# save the best model using pickle\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380ef04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
