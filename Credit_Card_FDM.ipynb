{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=5f971cb3b3719bc5c30bdc0f24f61210c2cf4f91b1312ba8b339258a5120c3a1\n",
      "  Stored in directory: /Users/neil/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x216 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Packages related to data importing, manipulation, exploratory data #analysis, data understanding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from termcolor import colored as cl # text customization\n",
    "#Packages related to data visualizaiton\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Setting plot sizes and type of plot\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.gray()\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, \n",
    "FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, \n",
    "OrdinalEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet,\n",
    "Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, \n",
    "export_graphviz, export\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor,\n",
    "RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor,\n",
    "AdaBoostClassifier, AdaBoostRegressor \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/Users/neil/Downloads/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTotal number of Trnsactions are 284807\u001b[0m\n",
      "\u001b[1mNumber of Normal Transactions are 284315\u001b[0m\n",
      "\u001b[1mNumber of fraudulent Transactions are 492\u001b[0m\n",
      "\u001b[1mPercentage of fraud Transactions is 0.17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Total_transactions = len(data)\n",
    "normal = len(data[data.Class == 0])\n",
    "fraudulent = len(data[data.Class == 1])\n",
    "fraud_percentage = round(fraudulent/normal*100, 2)\n",
    "print(cl('Total number of Transactions are {}'.format(Total_transactions), \n",
    "         attrs = ['bold']))\n",
    "print(cl('Number of Normal Transactions are {}'.format(normal), \n",
    "         attrs = ['bold']))\n",
    "print(cl('Number of fraudulent Transactions are {}'.format(fraudulent), \n",
    "         attrs = ['bold']))\n",
    "print(cl('Percentage of fraud Transactions is {}'.format(fraud_percentage), \n",
    "         attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prepn\n",
    "X = data.drop('Class', axis = 1).values\n",
    "y = data['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, \n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "DT.fit(X_train, y_train)\n",
    "y_pred = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Decision Tree model is 0.9991583957281328\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score\n",
    "                                                               (y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Decision Tree model is 0.7521367521367521\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68770,    18],\n",
       "       [   40,    88]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred, labels = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "KNN = KNeighborsClassifier(n_neighbors = n)\n",
    "KNN.fit(X_train, y_train)\n",
    "knn_yhat = KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the K-Nearest Neighbors model is 0.7949790794979079\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the K-Nearest Neighbors model is {}'.\n",
    "      format(f1_score(y_test, knn_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_yhat = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Logistic Regression model is 0.9989552498694062\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Logistic Regression model is {}'.\n",
    "      format(accuracy_score(y_test, lr_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Logistic Regression model is 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Logistic Regression model is {}'.\n",
    "      format(f1_score(y_test, lr_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_yhat = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Support Vector Machines model is 0.999318010331418\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Support Vector Machines model is {}'.\n",
    "      format(accuracy_score(y_test, svm_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Support Vector Machines model is 0.7813953488372093\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Support Vector Machines model is {}'.\n",
    "      format(f1_score(y_test, svm_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_yhat = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Random Forest model is 0.9991874165650937\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Random Forest model is {}'.\n",
    "      format(accuracy_score(y_test, rf_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Random Forest model is 0.7454545454545454\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Random Forest model is {}'.\n",
    "      format(f1_score(y_test, rf_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:16:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth = 4)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_yhat = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the XGBoost model is 0.999506645771664\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the XGBoost model is {}'.\n",
    "      format(accuracy_score(y_test, xgb_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the XGBoost model is 0.8495575221238937\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the XGBoost model is {}'.\n",
    "      format(f1_score(y_test, xgb_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion : Over all the models, XGB accuracy is best. \n",
    "#Hence, we deploy XGB model. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
